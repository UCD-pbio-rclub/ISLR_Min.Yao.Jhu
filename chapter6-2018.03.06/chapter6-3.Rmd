---
title: "chapter6-3"
author: "Min-Yao"
date: "2018年3月5日"
output: 
  html_document: 
    keep_md: yes
---

# Chapter 6 Lab 1: Subset Selection Methods

# Best Subset Selection

```{r}
library(ISLR)
#fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))

Hitters=na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))

library(leaps)
regfit.full=regsubsets(Salary~.,Hitters)
summary(regfit.full)

regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
reg.summary$rsq

#par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11], col="red",cex=2,pch=20)

plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)

#?plot.regsubsets
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
coef(regfit.full,6)
```

# Forward and Backward Stepwise Selection

```{r}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
regfit.bwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
summary(regfit.bwd)

coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
```

# Choosing Among Models

```{r}
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
test=(!train)

regfit.best=regsubsets(Salary~.,data=Hitters[train,],nvmax=19)
test.mat=model.matrix(Salary~.,data=Hitters[test,])

val.errors=rep(NA,19)
for(i in 1:19){
   coefi=coef(regfit.best,id=i)
   pred=test.mat[,names(coefi)]%*%coefi
   val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}

val.errors
which.min(val.errors)
coef(regfit.best,10)
```

```{r}
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
  }

regfit.best=regsubsets(Salary~.,data=Hitters,nvmax=19)
coef(regfit.best,10)
```

```{r}
k=10
set.seed(1)
folds=sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))

for(j in 1:k){
  best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,],nvmax=19)
  for(i in 1:19){
    pred=predict.regsubsets(best.fit,Hitters[folds==j,],id=i)
    cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
    }
}

mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')

reg.best=regsubsets(Salary~.,data=Hitters, nvmax=19)
coef(reg.best,11)
```

# Chapter 6 Lab 2: Ridge Regression and the Lasso

```{r}
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
```

# Ridge Regression

```{r}
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)

dim(coef(ridge.mod))

ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))

ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))

predict(ridge.mod,s=50,type="coefficients")[1:20,]
```

```{r}
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)

mean((mean(y[train])-y.test)^2)

ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)

ridge.pred=predict(ridge.mod,s=0,newx=x[test,])
mean((ridge.pred-y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod,s=0,type="coefficients")[1:20,]
```

```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam

ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)

out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
```


# The Lasso

```{r}
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)

set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)

out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

# 2. For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer.
i. More ﬂexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.
ii. More ﬂexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.
iii. Less ﬂexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.
iv. Less ﬂexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

## (a) The lasso, relative to least squares, is:

> iii. Less ﬂexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

## (b) Repeat (a) for ridge regression relative to least squares.

> iii. Less ﬂexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance. 

## (c) Repeat (a) for non-linear methods relative to least squares.

> ii. More ﬂexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

# 5. It is well-known that ridge regression tends to give similar coeﬃcient values to correlated variables, whereas the lasso may give quite different coeﬃcient values to correlated variables. We will now explore this property in a very simple setting. Suppose that n = 2, p = 2, x 11 = x 12 , x 21 = x 22 . Furthermore, suppose that y 1 +y 2 = 0 and x 11 +x 21 = 0 and x 12 +x 22 = 0, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: ˆβ 0 = 0.

## (a) Write out the ridge regression optimization problem in this setting.

## (b) Argue that in this setting, the ridge coeﬃcient estimates satisfy ˆβ 1 =ˆβ 2 .

## (c) Write out the lasso optimization problem in this setting.

## (d) Argue that in this setting, the lasso coeﬃcientsˆβ 1 and ˆβ 2 are not unique—in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions.

# 8. In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

## (a) Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector E of length n = 100.

```{r}
set.seed(12)
#?rnorm
X=rnorm(100, 10, 1)
E=rnorm(100, 5, 1)
```


## (b) Generate a response vector Y of length n = 100 according to the model Y = β 0 + β 1 X + β 2 X^2 + β 3 X^3 + E, where β 0 , β 1 , β 2 , and β 3 are constants of your choice.

```{r}
B0=1
B1=2
B2=3
B3=4
Y=B0+B1*X+B2*(X^2)+B3*(X^3)+E
```


## (c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X, X^2, . . . , X^10. What is the best model obtained according to Cp , BIC, and adjusted R 2 ? Show some plots to provide evidence for your answer, and report the coeﬃcients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y.

```{r}
library(leaps)
XYdata=data.frame(X,Y)
summary(XYdata)
plot(X,Y)

XYregfit.full=regsubsets(Y~poly(X,10),XYdata,nvmax = 10)
summary(XYregfit.full)

plot(XYregfit.full,scale="r2")
plot(XYregfit.full,scale="adjr2")
plot(XYregfit.full,scale="Cp")
plot(XYregfit.full,scale="bic")

XYreg.summary=summary(XYregfit.full)
plot(XYreg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(XYreg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(XYreg.summary$cp)
points(which.min(XYreg.summary$cp),XYreg.summary$cp[which.min(XYreg.summary$cp)],col="red",cex=2,pch=20)

plot(XYreg.summary$bic,xlab="Number of Variables",ylab="bic",type='l')
which.min(XYreg.summary$bic)
points(which.min(XYreg.summary$bic),XYreg.summary$bic[which.min(XYreg.summary$bic)],col="red",cex=2,pch=20)

plot(XYreg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
which.max(XYreg.summary$adjr2)
points(which.max(XYreg.summary$adjr2),XYreg.summary$adjr2[which.max(XYreg.summary$adjr2)], col="red",cex=2,pch=20)

coef(XYregfit.full,3)
coef(XYregfit.full,4)

```

> Cp and Adjusted RSq favor 4 variable model, but BIC favors 3 variable model.

## (d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?

```{r}
XYregfit.fwd=regsubsets(Y~poly(X,10),XYdata,nvmax = 10,method="forward")
summary(XYregfit.fwd)

plot(XYregfit.fwd,scale="r2")
plot(XYregfit.fwd,scale="adjr2")
plot(XYregfit.fwd,scale="Cp")
plot(XYregfit.fwd,scale="bic")

XYreg.summary=summary(XYregfit.fwd)
plot(XYreg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(XYreg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(XYreg.summary$cp)
points(which.min(XYreg.summary$cp),XYreg.summary$cp[which.min(XYreg.summary$cp)],col="red",cex=2,pch=20)

plot(XYreg.summary$bic,xlab="Number of Variables",ylab="bic",type='l')
which.min(XYreg.summary$bic)
points(which.min(XYreg.summary$bic),XYreg.summary$bic[which.min(XYreg.summary$bic)],col="red",cex=2,pch=20)

plot(XYreg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
which.max(XYreg.summary$adjr2)
points(which.max(XYreg.summary$adjr2),XYreg.summary$adjr2[which.max(XYreg.summary$adjr2)], col="red",cex=2,pch=20)

```

> the same results

## (e) Now ﬁt a lasso model to the simulated data, again using X, X^2 , . . . , X^10 as predictors. Use cross-validation to select the optimal value of λ. Create plots of the cross-validation error as a function of λ. Report the resulting coeﬃcient estimates, and discuss the results obtained.

```{r}
library(glmnet)
summary(XYdata)
dim(XYdata)
plot(X,Y)

grid=10^seq(10,-2,length=100)

x=model.matrix(Y~poly(X,10),XYdata)[,-1]
y=XYdata$Y

set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)

set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)

out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:11,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

> best subset selection

 (Intercept) poly(X, 10)1 poly(X, 10)2 poly(X, 10)3 
  4378.10082  11032.72208   1271.98179     49.78343 
  
  (Intercept)  poly(X, 10)1  poly(X, 10)2  poly(X, 10)3 poly(X, 10)10 
  4378.100817  11032.722081   1271.981792     49.783432      1.898334

> lasso

 (Intercept) poly(X, 10)1 poly(X, 10)2 
   4378.1008   10735.4124     974.6721



## (f) Now generate a response vector Y according to the model Y = β 0 + β 7 X^7 + E, and perform best subset selection and the lasso. Discuss the results obtained.

```{r}
B7=8
Y2=B0+B7*(X^7)+E
XY2data=data.frame(X,Y2)
summary(XY2data)
plot(X,Y2)

# best subset selection

library(leaps)

XY2regfit.full=regsubsets(Y2~poly(X,10),XY2data,nvmax = 10)
summary(XY2regfit.full)

plot(XY2regfit.full,scale="r2")
plot(XY2regfit.full,scale="adjr2")
plot(XY2regfit.full,scale="Cp")
plot(XY2regfit.full,scale="bic")

XY2reg.summary=summary(XY2regfit.full)
plot(XY2reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(XY2reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(XY2reg.summary$cp)
points(which.min(XY2reg.summary$cp),XY2reg.summary$cp[which.min(XY2reg.summary$cp)],col="red",cex=2,pch=20)

plot(XY2reg.summary$bic,xlab="Number of Variables",ylab="bic",type='l')
which.min(XY2reg.summary$bic)
points(which.min(XY2reg.summary$bic),XY2reg.summary$bic[which.min(XY2reg.summary$bic)],col="red",cex=2,pch=20)

plot(XY2reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
which.max(XY2reg.summary$adjr2)
points(which.max(XY2reg.summary$adjr2),XY2reg.summary$adjr2[which.max(XY2reg.summary$adjr2)], col="red",cex=2,pch=20)

coef(XY2regfit.full,8)
coef(XY2regfit.full,7)


# lasso
library(glmnet)

grid=10^seq(10,-2,length=100)

x=model.matrix(Y2~poly(X,10),XY2data)[,-1]
y=XY2data$Y2

set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)

set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)

out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:11,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

> best subset selection

  (Intercept)  poly(X, 10)1  poly(X, 10)2  poly(X, 10)3  poly(X, 10)4 
 9.129965e+07  5.498227e+08  1.887044e+08  3.715973e+07  3.839524e+06 
 poly(X, 10)5  poly(X, 10)6  poly(X, 10)7 poly(X, 10)10 
 2.013016e+05  6.641229e+03  1.090581e+02  1.898334e+00 
 
 (Intercept) poly(X, 10)1 poly(X, 10)2 poly(X, 10)3 poly(X, 10)4 poly(X, 10)5 
9.129965e+07 5.498227e+08 1.887044e+08 3.715973e+07 3.839524e+06 2.013016e+05 
poly(X, 10)6 poly(X, 10)7 
6.641229e+03 1.090581e+02 

> lasso

 (Intercept) poly(X, 10)1 poly(X, 10)2 poly(X, 10)3 
    91299650    537211391    176093109     24548447 


# 9. In this exercise, we will predict the number of applications received using the other variables in the College data set.

## (a) Split the data set into a training set and a test set.

```{r}
summary(College)
dim(College)
#?College

x=model.matrix(Apps~.,College)[,-1]
y=College$Apps
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

#set.seed(1)
#train=sample(1:nrow(College), nrow(College)/2)
#test=(-train)
#College.test=College[test]

```

## (b) Fit a linear model using least squares on the training set, and report the test error obtained.

```{r}
lm.fit=lm(y~x, subset=train)
mean((College$Apps-predict(lm.fit,College))[-train]^2)

```

> test MSE = 1108531

## (c) Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.

```{r}
library(glmnet)
grid=10^seq(10,-2,length=100)

ridge.mod=glmnet(x,y,alpha=0,lambda=grid)

dim(coef(ridge.mod))

ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)

set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam

ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)

```

> test MSE = 1037308

## (d) Fit a lasso model on the training set, with λ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coeﬃcient estimates.

```{r}
# lasso
library(glmnet)

lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)

set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)

out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:18,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

> test MSE = 1032128



# 11. We will now try to predict per capita crime rate in the Boston data set.

## (a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.

```{r}
library(MASS)
summary(Boston)

# best subset selection

library(leaps)

regfit.full=regsubsets(crim~.,Boston,nvmax = 13)
summary(regfit.full)

plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")

reg.summary=summary(regfit.full)
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(reg.summary$cp)
points(which.min(reg.summary$cp),reg.summary$cp[which.min(reg.summary$cp)],col="red",cex=2,pch=20)

plot(reg.summary$bic,xlab="Number of Variables",ylab="bic",type='l')
which.min(reg.summary$bic)
points(which.min(reg.summary$bic),reg.summary$bic[which.min(reg.summary$bic)],col="red",cex=2,pch=20)

plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
which.max(reg.summary$adjr2)
points(which.max(reg.summary$adjr2),reg.summary$adjr2[which.max(reg.summary$adjr2)], col="red",cex=2,pch=20)

coef(regfit.full,9)
coef(regfit.full,8)
coef(regfit.full,3)

# the lasso
library(glmnet)

grid=10^seq(10,-2,length=100)

x=model.matrix(crim~.,Boston)[,-1]
y=Boston$crim

set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)

set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)

out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:11,]
lasso.coef
lasso.coef[lasso.coef!=0]

# ridge regression
library(glmnet)

ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)

set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam

ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)

out=glmnet(x,y,alpha=0,lambda=grid)
ridge.coef=predict(out,type="coefficients",s=bestlam)[1:11,]
ridge.coef

```

> best subset selection,
Cp: 8; bic: 3; Adjusted RSq: 9

  (Intercept)            zn         indus           nox           dis 
 19.124636156   0.042788127  -0.099385948 -10.466490364  -1.002597606 
          rad       ptratio         black         lstat          medv 
  0.539503547  -0.270835584  -0.008003761   0.117805932  -0.180593877 
  (Intercept)            zn           nox           dis           rad 
 19.683127801   0.043293393 -12.753707757  -0.918318253   0.532616533 
      ptratio         black         lstat          medv 
 -0.310540942  -0.007922426   0.110173124  -0.174207166 
 (Intercept)          rad        black        lstat 
-0.372585457  0.488172386 -0.009471639  0.213595700 

> lasso: 8

(Intercept)          zn       indus        chas         nox          rm 
 9.26270091  0.03135641 -0.05102314 -0.51264890 -3.75545166  0.04132004 
        dis         rad     ptratio 
-0.60070039  0.49479389 -0.10750998 

> MSE = 38.3096

> ridge regression

 (Intercept)           zn        indus         chas          nox           rm 
14.776826098  0.041373639 -0.074977442 -0.733273998 -9.003406341  0.413309117 
         age          dis          rad          tax      ptratio 
 0.001260395 -0.915381715  0.537911180 -0.001370858 -0.234996727 

> MSE = 38.86819



## (b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross-validation, or some other reasonable alternative, as opposed to using training error.

```{r}
# Choosing Among Models
# validation set approach

set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Boston),rep=TRUE)
test=(!train)

regfit.best=regsubsets(crim~.,Boston[train,],nvmax = 13)
summary(regfit.best)
test.mat=model.matrix(crim~.,Boston[test,])

val.errors=rep(NA,13)
for(i in 1:13){
   coefi=coef(regfit.best,id=i)
   pred=test.mat[,names(coefi)]%*%coefi
   val.errors[i]=mean((Boston$crim[test]-pred)^2)
}

val.errors
which.min(val.errors)

coef(regfit.best,2)

# best model has 2 variables

# make a predict function
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
  }

regfit.best=regsubsets(crim~.,Boston,nvmax=13)
coef(regfit.best,2)

# Cross validation

k=10
set.seed(1)
folds=sample(1:k,nrow(Boston),replace=TRUE)
cv.errors=matrix(NA,k,13, dimnames=list(NULL, paste(1:13)))

for(j in 1:k){
  best.fit=regsubsets(crim~.,data=Boston[folds!=j,],nvmax=13)
  for(i in 1:13){
    pred=predict.regsubsets(best.fit,Boston[folds==j,],id=i)
    cv.errors[j,i]=mean((Boston$crim[folds==j]-pred)^2)
    }
}

mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')

reg.best=regsubsets(crim~.,data=Boston, nvmax=13)
coef(reg.best,12)
coef(reg.best,9)
```

> best model by validation set approach

(Intercept)         rad       lstat 
 -4.3814053   0.5228128   0.2372846 
 
> best model by Cross validation

> 12 v

  (Intercept)            zn         indus          chas           nox 
 16.985713928   0.044673247  -0.063848469  -0.744367726 -10.202169211 
           rm           dis           rad           tax       ptratio 
  0.439588002  -0.993556631   0.587660185  -0.003767546  -0.269948860 
        black         lstat          medv 
 -0.007518904   0.128120290  -0.198877768 
 
> 9 v

   (Intercept)            zn         indus           nox           dis 
 19.124636156   0.042788127  -0.099385948 -10.466490364  -1.002597606 
          rad       ptratio         black         lstat          medv 
  0.539503547  -0.270835584  -0.008003761   0.117805932  -0.180593877 

## (c) Does your chosen model involve all of the features in the data set? Why or why not?

> No
