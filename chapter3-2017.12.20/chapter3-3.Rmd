---
title: "chapter3-3"
author: "Min-Yao"
date: "2017年12月19日"
output: 
  html_document: 
    keep_md: yes
---

# Chapter 3 Lab: Linear Regression

```{r}
library(MASS)
library(ISLR)
```

# Simple Linear Regression
```{r}
#fix(Boston)
names(Boston)
#lm.fit=lm(medv~lstat)
lm.fit=lm(medv~lstat,data=Boston)
attach(Boston)
lm.fit=lm(medv~lstat)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="confidence")
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="prediction")
plot(lstat,medv)
abline(lm.fit)
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3,col="red")
plot(lstat,medv,col="red")
plot(lstat,medv,pch=20)
plot(lstat,medv,pch="+")
plot(1:20,1:20,pch=1:20)
par(mfrow=c(2,2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

# Multiple Linear Regression
```{r}
lm.fit=lm(medv~lstat+age,data=Boston)
summary(lm.fit)
lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)
library(car)
vif(lm.fit)
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
lm.fit1=update(lm.fit, ~.-age)
```


# Interaction Terms

```{r}
summary(lm(medv~lstat*age,data=Boston))
```


# Non-linear Transformations of the Predictors
```{r}
lm.fit2=lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
lm.fit=lm(medv~lstat)
anova(lm.fit,lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)
summary(lm(medv~log(rm),data=Boston))
```


# Qualitative Predictors
```{r}
#fix(Carseats)
names(Carseats)
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
attach(Carseats)
contrasts(ShelveLoc)
```


# Writing Functions
```{r}
#LoadLibraries
#LoadLibraries()
LoadLibraries=function(){
 library(ISLR)
 library(MASS)
 print("The libraries have been loaded.")
 }
LoadLibraries
LoadLibraries()
```

3. Suppose we have a data set with ﬁve predictors, X 1 = GPA, X 2 = IQ,
X 3 = Gender (1 for Female and 0 for Male), X 4 = Interaction between
GPA and IQ, and X 5 = Interaction between GPA and Gender. The
response is starting salary after graduation (in thousands of dollars).
Suppose we use least squares to ﬁt the model, and get ˆβ 0 = 50, ˆβ 1 =
20, ˆβ 2 = 0.07, ˆβ 3 = 35, ˆβ 4 = 0.01, ˆβ 5 = − 10.

salary = 50 + 20XGPA + 0.07XIQ + 35XGender + 0.01XGPAXIQ + (− 10)XGPAXGender

(a) Which answer is correct, and why?
i. For a ﬁxed value of IQ and GPA, males earn more on average
than females.

> No

ii. For a ﬁxed value of IQ and GPA, females earn more on
average than males.

> Yes

iii. For a ﬁxed value of IQ and GPA, males earn more on average
than females provided that the GPA is high enough.

> Yes

iv. For a ﬁxed value of IQ and GPA, females earn more on
average than males provided that the GPA is high enough.

> No

(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.

```{r}
Predict1 <- 50 + 20*4 + 0.07*110 + 35*1 + 0.01*4*110 + (-10)*4*1
Predict1

```

> 137.1 (in thousands of dollars)

(c) True or false: Since the coeﬃcient for the GPA/IQ interaction
term is very small, there is very little evidence of an interaction
eﬀect. Justify your answer.

> No, The evidence of an interaction eﬀect should based on p value.


9. This question involves the use of multiple linear regression on the
Auto data set.

(d) Use the plot() function to produce diagnostic plots of the linear
regression ﬁt. Comment on any problems you see with the ﬁt.
Do the residual plots suggest any unusually large outliers? Does
the leverage plot identify any observations with unusually high
leverage?

```{r}
lm.fit.auto=lm(mpg~.-name,data=Auto)
summary(lm.fit.auto)
par(mfrow=c(2,2)) 
plot(lm.fit.auto)

```
> Potential Problems

> 1. Non-linearity of the response-predictor relationships. 2. Not normal distribution. 3. Outliers. 4. High-leverage points.

(e) Use the * and : symbols to ﬁt linear regression models with
interaction eﬀects. Do any interactions appear to be statistically
signiﬁcant?

```{r}
names(Auto)

lm.fit.auto.2I=lm(mpg~(.-name)*(.-name),data=Auto)
summary(lm.fit.auto.2I)

lm.fit.auto.3I=lm(mpg~(.-name)*(.-name)*(.-name),data=Auto)
summary(lm.fit.auto.3I)

lm.fit.auto.4I=lm(mpg~(.-name)*(.-name)*(.-name)*(.-name),data=Auto)
summary(lm.fit.auto.4I)

lm.fit.auto.allI=lm(mpg~.-name + cylinders*displacement*horsepower*weight*acceleration*year*origin,data=Auto)
summary(lm.fit.auto.allI)
```

> 2 I: displacement:year, acceleration:year, acceleration:origin

> 3 I: weight:year:origin, horsepower:year:origin, horsepower:acceleration:origin, horsepower:acceleration:year,displacement:horsepower:year, cylinders:displacement:origin

> new 2I: displacement:horsepower, horsepower:acceleration, horsepower:origin, weight:origin, acceleration:origin

> Why different?

(f) Try a few diﬀerent transformations of the variables, such as
log(X), √X, X 2 . Comment on your ﬁndings.

```{r}
names(Auto)

lm.fit.auto=lm(mpg~.-name,data=Auto)
summary(lm.fit.auto)

lm.fit.auto.log=lm(mpg~.-name -cylinders -horsepower -acceleration + log(cylinders) + log(horsepower) + log(acceleration),data=Auto)
summary(lm.fit.auto.log)

lm.fit.auto.poly=lm(mpg~poly(cylinders+displacement+horsepower+weight+acceleration+year+origin,3),data=Auto)
summary(lm.fit.auto.poly)

```


10. This question should be answered using the Carseats data set.

(h) Is there evidence of outliers or high leverage observations in the
model from (e)?

> outliers are observations for which the response y i is unusual given the predictor x i . In contrast, observations with high leverage have an unusual value for x i . 

```{r}
#Carseats
summary(Carseats)
lm.fit.Carseats=lm(Sales~Price+Urban+US,data=Carseats)
summary(lm.fit.Carseats)
#?Carseats
```

```{r}
lm.fit.Carseats.new <- update(lm.fit.Carseats, ~ . - Urban)
summary(lm.fit.Carseats.new)
```

```{r}
confint(lm.fit.Carseats.new)
```

```{r}
plot(lm.fit.Carseats.new)

```
