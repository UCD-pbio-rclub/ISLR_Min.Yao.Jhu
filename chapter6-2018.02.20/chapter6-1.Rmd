---
title: "chapter6-1"
author: "Min-Yao"
date: "2018年2月18日"
output: 
  html_document: 
    keep_md: yes
---

# Chapter 6 Lab 1: Subset Selection Methods

# Best Subset Selection

```{r}
library(ISLR)
#fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))

Hitters=na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))

library(leaps)
regfit.full=regsubsets(Salary~.,Hitters)
summary(regfit.full)

regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
reg.summary$rsq

par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11], col="red",cex=2,pch=20)

plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)

#?plot.regsubsets
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
coef(regfit.full,6)
```


# Forward and Backward Stepwise Selection

```{r}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
regfit.bwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
summary(regfit.bwd)

coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
```

# 1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2, . . . , p predictors. Explain your answers:

## (a) Which of the three models with k predictors has the smallest training RSS?

> Best subset, because forward stepwise and backward stepwise selection are not guaranteed to ﬁnd the best possible model out of all 2^p models containing subsets of the p predictors. 

## (b) Which of the three models with k predictors has the smallest test RSS?

> We don't know. A number of techniques for adjusting the training error for the model size are available, like Cp, BIC, AIC and adjusted R^2. These approaches can be used to select among a set of models with diﬀerent numbers of variables. But, actually, we don't know which model has the real smallest test RSS unless we have real test data.

## (c) True or False:

### i. The predictors in the k-variable model identiﬁed by forward stepwise are a subset of the predictors in the (k+1)-variable model identiﬁed by forward stepwise selection.

> True.

### ii. The predictors in the k-variable model identiﬁed by backward stepwise are a subset of the predictors in the (k + 1)-variable model identiﬁed by backward stepwise selection.

> True.

### iii. The predictors in the k-variable model identiﬁed by back-ward stepwise are a subset of the predictors in the (k + 1)-variable model identiﬁed by forward stepwise selection.

> False. The best k-variable models and (k + 1)-variable models identiﬁed by forward stepwise selection and backward stepwise selection may be diﬀerent.

### iv. The predictors in the k-variable model identiﬁed by forward stepwise are a subset of the predictors in the (k+1)-variable model identiﬁed by backward stepwise selection.

> False. The best k-variable models and (k + 1)-variable models identiﬁed by forward stepwise selection and backward stepwise selection may be diﬀerent.

### v. The predictors in the k-variable model identiﬁed by best subset are a subset of the predictors in the (k + 1)-variable model identiﬁed by best subset selection.

> False. Because best subset may also remove any variables that no longer provide an improvement in the model ﬁt.

# 8. In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

## (a) Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector E of length n = 100.

```{r}
set.seed(12)
#?rnorm
X=rnorm(100, 10, 1)
E=rnorm(100, 5, 1)
```


## (b) Generate a response vector Y of length n = 100 according to the model Y = β 0 + β 1 X + β 2 X^2 + β 3 X^3 + E, where β 0 , β 1 , β 2 , and β 3 are constants of your choice.

```{r}
B0=1
B1=2
B2=3
B3=4
Y=B0+B1*X+B2*(X^2)+B3*(X^3)+E
```


## (c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X, X^2, . . . , X^10. What is the best model obtained according to Cp , BIC, and adjusted R 2 ? Show some plots to provide evidence for your answer, and report the coeﬃcients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y.

```{r}
library(leaps)
XYdata=data.frame(X,Y)
summary(XYdata)
plot(X,Y)

XYregfit.full=regsubsets(Y~poly(X,10),XYdata,nvmax = 10)
summary(XYregfit.full)

plot(XYregfit.full,scale="r2")
plot(XYregfit.full,scale="adjr2")
plot(XYregfit.full,scale="Cp")
plot(XYregfit.full,scale="bic")

XYreg.summary=summary(XYregfit.full)
plot(XYreg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(XYreg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(XYreg.summary$cp)
points(which.min(XYreg.summary$cp),XYreg.summary$cp[which.min(XYreg.summary$cp)],col="red",cex=2,pch=20)

plot(XYreg.summary$bic,xlab="Number of Variables",ylab="bic",type='l')
which.min(XYreg.summary$bic)
points(which.min(XYreg.summary$bic),XYreg.summary$bic[which.min(XYreg.summary$bic)],col="red",cex=2,pch=20)

plot(XYreg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
which.max(XYreg.summary$adjr2)
points(which.max(XYreg.summary$adjr2),XYreg.summary$adjr2[which.max(XYreg.summary$adjr2)], col="red",cex=2,pch=20)

coef(XYregfit.full,3)
coef(XYregfit.full,4)

```

> Cp and Adjusted RSq favor 4 variable model, but BIC favors 3 variable model.

## (d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?

```{r}
XYregfit.fwd=regsubsets(Y~poly(X,10),XYdata,nvmax = 10,method="forward")
summary(XYregfit.fwd)

plot(XYregfit.fwd,scale="r2")
plot(XYregfit.fwd,scale="adjr2")
plot(XYregfit.fwd,scale="Cp")
plot(XYregfit.fwd,scale="bic")

XYreg.summary=summary(XYregfit.fwd)
plot(XYreg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(XYreg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(XYreg.summary$cp)
points(which.min(XYreg.summary$cp),XYreg.summary$cp[which.min(XYreg.summary$cp)],col="red",cex=2,pch=20)

plot(XYreg.summary$bic,xlab="Number of Variables",ylab="bic",type='l')
which.min(XYreg.summary$bic)
points(which.min(XYreg.summary$bic),XYreg.summary$bic[which.min(XYreg.summary$bic)],col="red",cex=2,pch=20)

plot(XYreg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
which.max(XYreg.summary$adjr2)
points(which.max(XYreg.summary$adjr2),XYreg.summary$adjr2[which.max(XYreg.summary$adjr2)], col="red",cex=2,pch=20)

```

> the same results



## (e) Now ﬁt a lasso model to the simulated data, again using X, X^2 , . . . , X^10 as predictors. Use cross-validation to select the optimal value of λ. Create plots of the cross-validation error as a function of λ. Report the resulting coeﬃcient estimates, and discuss the results obtained.

## (f) Now generate a response vector Y according to the model Y = β 0 + β 7 X^7 + E, and perform best subset selection and the lasso. Discuss the results obtained.