---
title: "chapter3-2"
author: "Min-Yao"
date: "2017年12月12日"
output: 
  html_document: 
    keep_md: yes
---

```{r}
library(MASS)
library(ISLR)
```


# Multiple Linear Regression

```{r}

lm.fit=lm(medv~lstat+age,data=Boston)
summary(lm.fit)
lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)
library(car)
vif(lm.fit)
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
lm.fit1=update(lm.fit, ~.-age)
```

# 1. Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coeﬃcients of the linear model.

>  the null hypothesis is 
H 0 : β 1 = β 2 = · · · = β p = 0
there is no relationship between the response and the predictors

> Intercept
the null model: a model that contains an intercept but no predictors. .
null hypothesis = no advertising -> no sales
Reject, because small p-value

> TV
for a given amount of radio and newspaper advertising, spending an additional $1,000 on TV advertising leads to an increase in sales by approximately 46 units. 
null hypothesis = add TV -> no sale changes
Reject, because small p-value

> radio
for a given amount of TV and newspaper advertising, spending an additional $1,000 on radio advertising leads to an increase in sales by approximately 189 units. 
null hypothesis = add TV -> no sale changes
Reject, because small p-value

> newspaper
for a given amount of TV and radio advertising, spending an additional $1,000 on newspaper advertising leads to an decrease in sales by approximately 1 units. 
null hypothesis = add newspaper -> no sale changes
accept, because large p-value

# 9. This question involves the use of multiple linear regression on the Auto data set.

(a) Produce a scatterplot matrix which includes all of the variables
in the data set.

```{r}
pairs (Auto)
```


(b) Compute the matrix of correlations between the variables using
the function cor(). You will need to exclude the name variable, which is qualitative.

```{r}
#?cor()
summary(Auto)
str(Auto)
autononame<-Auto[,-9]
#autononame
#Auto

cor(autononame)

```


(c) Use the lm() function to perform a multiple linear regression
with mpg as the response and all other variables except name as
the predictors. Use the summary() function to print the results.
Comment on the output. For instance:

```{r}
lm.fit.auto=lm(mpg~.-name,data=Auto)
summary(lm.fit.auto)
```


i. Is there a relationship between the predictors and the re-
sponse?

> Yes, because p-value: < 2.2e-16

ii. Which predictors appear to have a statistically signiﬁcant
relationship to the response?

> displacement, because p-value: 0.00844
> weight, because p-value: < 2e-16
> year, because p-value: < 2e-16
> origin, because p-value: 4.67e-07


iii. What does the coeﬃcient for the year variable suggest?

> for a given amount of other predictors, adding one year leads to an increase in mpg by approximately 0.75 units.

# 10. This question should be answered using the Carseats data set.
(a) Fit a multiple regression model to predict Sales using Price,
Urban, and US.

```{r}
#Carseats
summary(Carseats)
lm.fit.Carseats=lm(Sales~Price+Urban+US,data=Carseats)
summary(lm.fit.Carseats)
#?Carseats
```


(b) Provide an interpretation of each coeﬃcient in the model. Be
careful—some of the variables in the model are qualitative!

> Intercept: the null model: a model that contains an intercept but no predictors. 
null hypothesis = no predictors -> no sales
Reject, because small p-value
*Intercept is different from 0 or not

> Price(Price company charges for car seats at each site)
For a given amount of other predictors, increasing one unit of Price leads to an decrease in Sales by approximately 54 units.

> Urban(A factor with levels No and Yes to indicate whether the store is in an urban or rural location)
For a given amount of other predictors, whether the store is in an urban or rural location doesn't affect sales.

> US(A factor with levels No and Yes to indicate whether the store is in the US or not)
For a given amount of other predictors, whether the store is in the US or not affect sales. If the store is in the US, Sales increases 1200 units.

(c) Write out the model in equation form, being careful to handle
the qualitative variables properly.

> Sales = β 0 + β 1 Price + β 2 Urban + β 3 US + E.

> Sales = 13.04 -0.05 Price -0.02 Urban + 1.20 US + E.

(d) For which of the predictors can you reject the null hypothesis
H 0 : β j = 0?

> Intercept, p value:< 2e-16

> Price, p value:< 2e-16

> USYes, p value:4.86e-06

(e) On the basis of your response to the previous question, ﬁt a
smaller model that only uses the predictors for which there is
evidence of association with the outcome.

```{r}
lm.fit.Carseats.new <- update(lm.fit.Carseats, ~ . - Urban)
summary(lm.fit.Carseats.new)
```


(f) How well do the models in (a) and (e) ﬁt the data?

> (a) Adjusted R-squared:  0.2335
> (e) Adjusted R-squared:  0.2354

(g) Using the model from (e), obtain 95 % conﬁdence intervals for
the coeﬃcient(s).

```{r}
confint(lm.fit.Carseats.new)
```


(h) Is there evidence of outliers or high leverage observations in the
model from (e)?

> outliers are observations for which the response y i is unusual given the predictor x i . In contrast, observations with high leverage have an unusual value for x i . 

```{r}
plot(lm.fit.Carseats.new)

```


# 15. This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

(a) For each predictor, ﬁt a simple linear regression model to predict
the response. Describe your results. In which of the models is
there a statistically signiﬁcant association between the predictor
and the response? Create some plots to back up your assertions.

```{r}
library(MASS)
summary(Boston)
names(Boston)
#?Boston
```

> faster method from last week

```{r}
library(psych)
library(tidyverse)

data(Boston)
boston <- as_tibble(Boston)
boston
predictors <- colnames(boston)[-1]
lmfits <- map(predictors,function(x) lm(crim ~ get(x), data=boston))
lmsummaries <- lapply(lmfits,summary)
names(lmsummaries) <- predictors
lmsummaries

```

> slow method

```{r}
attach(Boston)
lm.fit.zn=lm(crim~zn)
summary(lm.fit.zn)
lm.fit.indus=lm(crim~indus)
summary(lm.fit.indus)
lm.fit.chas=lm(crim~chas)
summary(lm.fit.chas)
lm.fit.nox=lm(crim~nox)
summary(lm.fit.nox)
lm.fit.rm=lm(crim~rm)
summary(lm.fit.rm)
lm.fit.age=lm(crim~age)
summary(lm.fit.age)
lm.fit.dis=lm(crim~dis)
summary(lm.fit.dis)
lm.fit.rad=lm(crim~rad)
summary(lm.fit.rad)
lm.fit.tax=lm(crim~tax)
summary(lm.fit.tax)
lm.fit.ptratio=lm(crim~ptratio)
summary(lm.fit.ptratio)
lm.fit.black=lm(crim~black)
summary(lm.fit.black)
lm.fit.lstat=lm(crim~lstat)
summary(lm.fit.lstat)
lm.fit.medv=lm(crim~medv)
summary(lm.fit.medv)
```


(b) Fit a multiple regression model to predict the response using
all of the predictors. Describe your results. For which predictors
can we reject the null hypothesis H 0 : β j = 0?

```{r}
summary(Boston)

lm.fit.all=lm(crim~.,data=Boston)
summary(lm.fit.all)
#?Boston
```

> zn: proportion of residential land zoned for lots over 25,000 sq.ft.

> dis: weighted mean of distances to five Boston employment centres.

> rad: index of accessibility to radial highways.

> black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

> medv: median value of owner-occupied homes in \$1000s.

(c) How do your results from (a) compare to your results from (b)?
Create a plot displaying the univariate regression coeﬃcients
from (a) on the x-axis, and the multiple regression coeﬃcients
from (b) on the y-axis. That is, each predictor is displayed as a
single point in the plot. Its coeﬃcient in a simple linear regres-
sion model is shown on the x-axis, and its coeﬃcient estimate
in the multiple linear regression model is shown on the y-axis.

```{r}

predictors <- colnames(boston)[-1]
coefficients.single <- map(predictors,function(x) coefficients(lm(crim ~ get(x), data=boston))[2])
coefficients.single

##################################

coefficients(lm.fit.zn)
coefficients(lm.fit.all)

coefficients.a = c(coefficients(lm.fit.zn)[2],
                   coefficients(lm.fit.indus)[2],
                   coefficients(lm.fit.chas)[2],
                   coefficients(lm.fit.nox)[2],
                   coefficients(lm.fit.rm)[2],
                   coefficients(lm.fit.age)[2],
                   coefficients(lm.fit.dis)[2],
                   coefficients(lm.fit.rad)[2],
                   coefficients(lm.fit.tax)[2],
                   coefficients(lm.fit.ptratio)[2],
                   coefficients(lm.fit.black)[2],
                   coefficients(lm.fit.lstat)[2],
                   coefficients(lm.fit.medv)[2])
coefficients.a

names(coefficients.a)
name = c(names(coefficients.a))
name

coefficients.b = coefficients(lm.fit.all)[2:14]
coefficients.b

final.data.fram = data.frame(x=coefficients.a, y= coefficients.b, names = name)
final.data.fram

library(ggplot2)

ggplot(final.data.fram, aes(x,y, color =names)) + 
  geom_point() + 
  geom_text(aes(label=names, color =names))
```


(d) Is there evidence of non-linear association between any of the
predictors and the response? To answer this question, for each
predictor X, ﬁt a model of the form
Y = β 0 + β 1 X + β 2 X2 + β 3 X3 + E.


> faster method based on last week's codes

```{r}
#library(psych)
#library(tidyverse)

#data(Boston)
#boston <- as_tibble(Boston)
head(boston)

predictors <- colnames(boston)[-1]
predictors
predictors2 <- predictors [-3]
predictors2

head(Boston)

polylmfits <- map(predictors2,
                  function(i) lm(crim ~ poly(Boston[,i],3),data = Boston))

polylmsummaries <- lapply(polylmfits,summary)
names(polylmsummaries) <- predictors2
polylmsummaries

```

> slow method

```{r}

attach(Boston)
poly.lm.fit.zn=lm(crim~poly(zn,3))
summary(poly.lm.fit.zn)

poly.lm.fit.indus=lm(crim~poly(indus,3))
summary(poly.lm.fit.indus)

#poly.lm.fit.chas=lm(crim~poly(chas,3))
#summary(poly.lm.fit.chas)
poly.lm.fit.chas=lm(crim~poly(chas,1))
summary(poly.lm.fit.chas)

poly.lm.fit.nox=lm(crim~poly(nox,3))
summary(poly.lm.fit.nox)

poly.lm.fit.rm=lm(crim~poly(rm,3))
summary(poly.lm.fit.rm)

poly.lm.fit.age=lm(crim~poly(age,3))
summary(poly.lm.fit.age)

poly.lm.fit.dis=lm(crim~poly(dis,3))
summary(poly.lm.fit.dis)

poly.lm.fit.rad=lm(crim~poly(rad,3))
summary(poly.lm.fit.rad)

poly.lm.fit.tax=lm(crim~poly(tax,3))
summary(poly.lm.fit.tax)

poly.lm.fit.ptratio=lm(crim~poly(ptratio,3))
summary(poly.lm.fit.ptratio)

poly.lm.fit.black=lm(crim~poly(black,3))
summary(poly.lm.fit.black)

poly.lm.fit.lstat=lm(crim~poly(lstat,3))
summary(poly.lm.fit.lstat)

poly.lm.fit.medv=lm(crim~poly(medv,3))
summary(poly.lm.fit.medv)
```
